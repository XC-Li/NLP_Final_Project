{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Analyzing Review Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review_reduced.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/2/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in json package has a `loads()` function that converts a JSON string into a Python dictionary.  We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` library, but is *substantially* faster (at the cost of non-robust handling of malformed json).  We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review_reduced.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'votes': {'funny': 0, 'useful': 0, 'cool': 0},\n",
       " 'user_id': 'Qrs3EICADUKNFoUq2iHStA',\n",
       " 'review_id': '_ePLBPrkrf4bhyiKWEn4Qg',\n",
       " 'stars': 1,\n",
       " 'date': '2013-04-19',\n",
       " 'text': \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
       " 'type': 'review',\n",
       " 'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn will want the labels in a separate data structure, so let's pull those out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag_of_words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a linear model predicting the star rating based on the count of the words in each document (bag-of-words model).  Use a [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) or [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) to produce a feature matrix giving the counts of each word in each review.  Feed this in to linear model, such as `Ridge` or `SGDRegressor`, to predict the number of stars from each review.\n",
    "\n",
    "**Hints**:\n",
    "1. Don't forget to use tokenization!  This is important for good performance but it is also the most expensive step.  Try vectorizing as a first initial step and then running grid-search and cross-validation only on of this pre-processed data.  `CountVectorizer` has to memorize the mapping between words and the index to which it is assigned.  This is linear in the size of the vocabulary.  The `HashingVectorizer` does not have to remember this mapping and will lead to much smaller models.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [row['text'] for row in data]\n",
    "X = CountVectorizer().fit_transform(text)\n",
    "\n",
    "# Now, this can be run with many different parameters\n",
    "# without needing to retrain the vectorizer:\n",
    "model.fit(X, stars, hyperparameter=something)\n",
    "```\n",
    "\n",
    "2. Try choosing different values for `min_df` (minimum document frequency cutoff) and `max_df` in `CountVectorizer`.  Setting `min_df` to zero admits rare words which might only appear once in the entire corpus.  This is both prone to overfitting and makes your data unmanageably large.  Don't forget to use cross-validation or to select the right value.  Notice that `HashingVectorizer` doesn't support `min_df`  and `max_df`.  However, it's not hard to roll your own transformer that solves for these.\n",
    "\n",
    "3. Try using [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) or [`RidgeCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).  If the memory footprint is too big, try switching to [Stochastic Gradient Descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) You might find that even ordinary linear regression fails due to the data size.  Don't forget to use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to determine the regularization parameter!  How do the regularization parameter `alpha` and the values of `min_df` and `max_df` from `CountVectorizer` change the answer?\n",
    "\n",
    "4. You will likely pick up several hyperparameters between the tokenization step and the regularization of the estimator.  While is is more strictly correct to do a grid search over all of them at once, this can take a long time. Quite often, doing a grid search over a single hyperparameter at a time can produce similar results.  Alternatively, the grid search may be done over a smaller subset of the data, as long as it is representative of the whole.\n",
    "\n",
    "5. Finally, assemble a pipeline that will transform the data from records all the way to predictions.  This will allow you to submit its predict method to the grader for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'votes': {'funny': 0, 'useful': 0, 'cool': 0},\n",
       " 'user_id': 'Qrs3EICADUKNFoUq2iHStA',\n",
       " 'review_id': '_ePLBPrkrf4bhyiKWEn4Qg',\n",
       " 'stars': 1,\n",
       " 'date': '2013-04-19',\n",
       " 'text': \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
       " 'type': 'review',\n",
       " 'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import base\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import RidgeCV, LinearRegression, SGDRegressor, Ridge\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):    \n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        selected = []\n",
    "        for row in X:\n",
    "            selected.append(row[self.col_name])\n",
    "        return selected\n",
    "\n",
    "text = ColumnSelectTransformer(col_name='text').transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = CountVectorizer(min_df=0.002, max_df=0.6, stop_words='english').fit_transform(text)\n",
    "X = HashingVectorizer(stop_words='english').fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6064543490546254\n",
      "ET: 55.463743925094604\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=9527)\n",
    "start = time.time()\n",
    "ridge = RidgeCV(alphas=[1,10], cv=3)\n",
    "ridge.fit(X_train, y_train)\n",
    "print (ridge.score(X_test,y_test))\n",
    "print ('ET:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cs', ColumnSelectTransformer(col_name='text')), ('vectorizer', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_est = Pipeline([\n",
    "    # Column selector (remember the ML project?)\n",
    "    ('cs', ColumnSelectTransformer(col_name='text')),('vectorizer', HashingVectorizer(stop_words='english')), \n",
    "    ('regressor', Ridge(alpha=1))])\n",
    "bag_of_words_est.fit(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is key for good linear regression. Previously, we used the count as the normalization scheme.  Add in a normalization transformer to your pipeline to improve the score.  Try some of these:\n",
    "\n",
    "1. You can use the \"does this word present in this document\" as a normalization scheme, which means the values are always 1 or 0.  So we give no additional weight to the presence of the word multiple times.\n",
    "\n",
    "2. Try using the log of the number of counts (or more precisely, $log(x+1)$). This is often used because we want the repeated presence of a word to count for more but not have that effect tapper off.\n",
    "\n",
    "3. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a common normalization scheme used in text processing.  Use the [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer). There are options for using `idf` and taking the logarithm of `tf`.  Do these significantly affect the result?\n",
    "\n",
    "Finally, if you can't decide which one is better, don't forget that you can combine models with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idf = TfidfTransformer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5913948215389855\n",
      "ET: 56.083779096603394\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_idf,y,test_size=0.3,random_state=9527)\n",
    "start = time.time()\n",
    "ridge = RidgeCV(alphas=[1,10], cv=3)\n",
    "ridge.fit(X_train, y_train)\n",
    "print (ridge.score(X_test,y_test))\n",
    "print ('ET:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cs', ColumnSelectTransformer(col_name='text')), ('vectorizer', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "normalized_est = Pipeline([\n",
    "    ('cs', ColumnSelectTransformer(col_name='text')),('vectorizer', HashingVectorizer(stop_words='english')),('tfidf', TfidfTransformer(norm='l2')),\n",
    "    ('regressor', Ridge(alpha=1))])\n",
    "normalized_est.fit(data, stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a bigram model, we'll consider both single words and pairs of consecutive words that appear.  This is going to be a much higher dimensional problem (large $p$) so you should be careful about overfitting.\n",
    "\n",
    "Sometimes, reducing the dimension can be useful.  Because we are dealing with a sparse matrix, we have to use [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD).  If we reduce the dimensions, we can use a more sophisticated models than linear ones.\n",
    "\n",
    "As before, memory problems can crop up due to the engineering constraints. Playing with the number of features, using the `HashingVectorizer`, incorporating `min_df` and `max_df` limits, and handling stop-words in some way are all methods of addressing this issue. If you are using `CountVectorizer`, it is possible to run it with a fixed vocabulary (based on a training run, for instance). Check the documentation.\n",
    "\n",
    "**A side note on multi-stage model evaluation:** When your model consists of a pipeline with several stages, it can be worthwhile to evaluate which parts of the pipeline have the greatest impact on the overall accuracy (or other metric) of the model. This allows you to focus your efforts on improving the important algorithms, and leaving the rest \"good enough\".\n",
    "\n",
    "One way to accomplish this is through ceiling analysis, which can be useful when you have a training set with ground truth values at each stage. Let's say you're training a model to extract image captions from websites and return a list of names that were in the caption. Your overall accuracy at some point reaches 70%. You can try manually giving the model what you know are the correct image captions from the training set, and see how the accuracy improves (maybe up to 75%). Alternatively, giving the model the perfect name parsing for each caption increases accuracy to 90%. This indicates that the name parsing is a much more promising target for further work, and the caption extraction is a relatively smaller factor in the overall performance.\n",
    "\n",
    "If you don't know the right answers at different stages of the pipeline, you can still evaluate how important different parts of the model are to its performance by changing or removing certain steps while keeping everything else constant. You might try this kind of analysis to determine how important adding stopwords and stemming to your NLP model actually is, and how that importance changes with parameters like the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ET: 596.366409778595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import time\n",
    "start = time.time()\n",
    "bigram_est = Pipeline([\n",
    "    ('cs', ColumnSelectTransformer(col_name='text')),('vectorizer', HashingVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "    ('trunc', TruncatedSVD(n_components=300, algorithm='randomized', n_iter=5, random_state=9527)),('regressor', Ridge(alpha=1))])\n",
    "bigram_est.fit(data, stars)\n",
    "print ('ET:', time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## food_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look over all reviews of restaurants.  You can determine which businesses are restaurants by looking in the `yelp_train_academic_dataset_business.json.gz` file from the ml project or downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    business_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of this file corresponds to a single business.  The category key gives a list of categories for each; take all where \"Restaurants\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_ids = [x['business_id'] for x in business_data if 'Restaurants' in x['categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(restaurant_ids) == 12876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"business_id\" here is the same as in the review data.  Use this to extract the review text for all reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_reviews = [x['text'] for x in data if x['business_id'] in restaurant_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574278"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(restaurant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this if using the updated data file\n",
    "#assert len(restaurant_reviews) == 143361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(restaurant_reviews, open('restaurant_reviews.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. We can think of the corpus as defining an empirical distribution over all *n*-grams.  We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words. Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is the probability of the bigram $w_1 w_2$, then we want to look at word pairs $w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ \\frac{p(w_1 w_2)}{p(w_1) p(w_2)} $$\n",
    "\n",
    "is high.  Return the top 100 (mostly food) bigrams with this statistic with the 'right' prior factor (see below).\n",
    "\n",
    "Estimating the probabilities is simply a matter of counting, and there are number of approaches that will work.  One is to use one of the tokenizers to count up how many times each word and each bigram appears in each review, and then sum those up over all reviews.  You might want to know that the `CountVectorizer` has a `.get_feature_names()` method which gives the string associated with each column.  (Question for thought: Why doesn't the `HashingVectorizer` have a similar method?)\n",
    "\n",
    "*Questions:* This statistic is a ratio and problematic when the denominator is small.  We can fix this by applying Bayesian smoothing to $p(w)$ (i.e. mixing the empirical distribution with the uniform distribution over the vocabulary).\n",
    "\n",
    "1. How does changing this smoothing parameter affect the word pairs you get qualitatively?\n",
    "\n",
    "2. We can interpret the smoothing parameter as adding a constant number of occurrences of each word to our distribution.  Does this help you determine set a reasonable value for this 'prior factor'?\n",
    "\n",
    "3. For fun: also check out [Amazon's Statistically Improbable Phrases](http://en.wikipedia.org/wiki/Statistically_Improbable_Phrases).\n",
    "\n",
    "*Implementation note:*\n",
    "As you adjust the size of the Bayesian smoothing parameter, you will notice first nonsense phrases being removed and then legitimate bigrams being removed, leaving you with only generic bigrams.  The goal is to find a value of the smoothing parameter between these two transitions.\n",
    "\n",
    "The reference solution is not an aggressive filterer: it errors in favor of leaving apparently nonsensical words. On further consideration, many of these are actually somewhat meaningful. The smoothing parameter chosen in the reference solution is equivalent to giving each word 30 previous appearances prior to considering this data.  This was chosen by generating a list of bigrams for a range of smoothing parameters and seeing how many of the bigrams were shared between neighboring values.  When the shared fraction reached 95%, we judged the solution to have converged.  Note that `min_df` should not be set too high, where it could exclude these borderline words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english',min_df=7)\n",
    "words_vectorizer = CountVectorizer(ngram_range=(1,1),stop_words='english',min_df=5)\n",
    "\n",
    "bigrams = bigram_vectorizer.fit_transform(restaurant_reviews)\n",
    "words = words_vectorizer.fit_transform(restaurant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_cnt = bigrams.sum(axis=0)\n",
    "w_cnt = words.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = dict(zip(bigram_vectorizer.get_feature_names(), np.array(bi_cnt)[0]))\n",
    "word_dict = dict(zip(words_vectorizer.get_feature_names(), np.array(w_cnt)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_df = pd.DataFrame()\n",
    "calc_df['bigram'] = bigram_vectorizer.get_feature_names()\n",
    "calc_df['word1'] = [x.split()[0] for x in bigram_vectorizer.get_feature_names()]\n",
    "calc_df['word2'] = [x.split()[1] for x in bigram_vectorizer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_cnt = [word_dict.get(calc_df.loc[i]['word1'],0) for i in range(calc_df.shape[0])]\n",
    "word2_cnt = [word_dict.get(calc_df.loc[i]['word2'],0) for i in range(calc_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc_df['word1_cnt'] = word1_cnt\n",
    "#calc_df['word2_cnt'] = word2_cnt\n",
    "calc_df['bi_cnt'] = np.array(bi_cnt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calc_df[calc_df['bi_cnt'] >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['prob'] = df['bi_cnt']/((df['word1_cnt']+90)*(df['word2_cnt']+90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word1_cnt</th>\n",
       "      <th>word2_cnt</th>\n",
       "      <th>bi_cnt</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177101</th>\n",
       "      <td>ezzyujdouig4p gyb3pv_a</td>\n",
       "      <td>ezzyujdouig4p</td>\n",
       "      <td>gyb3pv_a</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>0.002777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243991</th>\n",
       "      <td>hodge podge</td>\n",
       "      <td>hodge</td>\n",
       "      <td>podge</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>0.002586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243324</th>\n",
       "      <td>himal chuli</td>\n",
       "      <td>himal</td>\n",
       "      <td>chuli</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>0.002516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244013</th>\n",
       "      <td>hoity toity</td>\n",
       "      <td>hoity</td>\n",
       "      <td>toity</td>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0.002512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432743</th>\n",
       "      <td>roka akor</td>\n",
       "      <td>roka</td>\n",
       "      <td>akor</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0.002451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177102</th>\n",
       "      <td>f_5_unx wrafcxuakbzrdw</td>\n",
       "      <td>f_5_unx</td>\n",
       "      <td>wrafcxuakbzrdw</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>0.002450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273704</th>\n",
       "      <td>knick knacks</td>\n",
       "      <td>knick</td>\n",
       "      <td>knacks</td>\n",
       "      <td>111</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419768</th>\n",
       "      <td>reina pepiada</td>\n",
       "      <td>reina</td>\n",
       "      <td>pepiada</td>\n",
       "      <td>83</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>0.002421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94301</th>\n",
       "      <td>cien agaves</td>\n",
       "      <td>cien</td>\n",
       "      <td>agaves</td>\n",
       "      <td>88</td>\n",
       "      <td>76</td>\n",
       "      <td>71</td>\n",
       "      <td>0.002403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38898</th>\n",
       "      <td>baskin robbins</td>\n",
       "      <td>baskin</td>\n",
       "      <td>robbins</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        bigram          word1           word2  word1_cnt  \\\n",
       "177101  ezzyujdouig4p gyb3pv_a  ezzyujdouig4p        gyb3pv_a         92   \n",
       "243991             hodge podge          hodge           podge         55   \n",
       "243324             himal chuli          himal           chuli         57   \n",
       "244013             hoity toity          hoity           toity         61   \n",
       "432743               roka akor           roka            akor         48   \n",
       "177102  f_5_unx wrafcxuakbzrdw        f_5_unx  wrafcxuakbzrdw         44   \n",
       "273704            knick knacks          knick          knacks        111   \n",
       "419768           reina pepiada          reina         pepiada         83   \n",
       "94301              cien agaves           cien          agaves         88   \n",
       "38898           baskin robbins         baskin         robbins         74   \n",
       "\n",
       "        word2_cnt  bi_cnt      prob  \n",
       "177101         92      92  0.002777  \n",
       "243991         54      54  0.002586  \n",
       "243324         56      54  0.002516  \n",
       "244013         55      55  0.002512  \n",
       "432743         46      46  0.002451  \n",
       "177102         44      44  0.002450  \n",
       "273704         98      92  0.002435  \n",
       "419768         70      67  0.002421  \n",
       "94301          76      71  0.002403  \n",
       "38898          73      64  0.002394  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='prob',ascending=False,inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100 = list(df['bigram'][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
